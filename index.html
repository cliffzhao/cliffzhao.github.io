---
layout: default
title: Yan Zhao
---
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<!-- <link rel="stylesheet" type="text/css" href="/css/main.css"> -->
<div class="blurb">
	<h1>Yan Zhao (赵岩)</h1>

    
    <h2>About me</h2>

    <p>
		I obtained my Ph.D. in Computer Science at The Ohio State University, advised by Prof. <a href="http://web.cse.ohio-state.edu/~dwang/">DeLiang Wang</a> at <a href="http://web.cse.ohio-state.edu/~dwang/pnl/">Perception and Neurodynamics Laboratory (PNL)</a>. Now I'm working as a Research Scientist at ByteDance.
	</p>

    <p>
		My research interest focuses on <I>speech enhancement/separation</I>, <I>audio processing</I> and <I>machine learning</I>.
	</p>
	
	<div align="left">
		<a href="https://www.linkedin.com/in/yan-zhao-8785a356/"><i class="fa fa-linkedin-square fa-3x"></i></a> <a href="https://scholar.google.com/citations?user=cHzLMDEAAAAJ&hl=zh-CN" target="_blank"><i class="ai ai-google-scholar-square ai-3x"></i></a>
	</div>
	
    <h2>Contact</h2>
    <ul>
      	<p>
		<i class="fa fa-envelope"></i> Email: </br>
		  [lastname] [dot] 836 [at] osu [dot] edu </br> 
		  [lastname] [dot] [firstname] [at] bytedance [dot] com
	</p>
      
    </ul>


    <h2>News</h2>
    <ul>

    <li> ...

    </ul>
	
    <h2>Experience</h2>
    <ul>
	      <li>
          <strong>Jul. 2020 - present</strong>: Research Scientist at SAMI (Speech, Audio & Music Intelligence), ByteDance (Mountain View, CA)
	      </li>

        <li>
          <strong>May 2019 - Aug. 2019</strong>: Applied Scientist Intern at AWS, Amazon (East Palo Alto, CA)
	      </li>

        <li>
          <strong>May 2018 - Aug. 2018</strong>: Research Intern at Machine Intelligence Technology, DAMO Academy, Alibaba Group (Bellevue, WA)
        </li>

        <li>
          <strong>May 2017 - Aug. 2017</strong>: Research Intern at Signal Processing Research Department, Starkey Hearing Technologies (Eden Prairie, MN)
        </li>
    </ul>
	
    <h2>Teaching</h2>
    <ul>
      <li>
        <strong>Instructor</strong>: CSE 1223: Introduction to Computer Programming in Java, Fall 2018, OSU
      </li>
    </ul>
    
    <h2>Publications</h2>
    <ul>
		
	  <li> Liu H., Kong Q., Tian Q., <strong>Zhao Y.</strong>, Wang D.L., Huang C., and Wang Y. (2021): <a href="https://arxiv.org/pdf/2109.13731.pdf" target="_blank"><U>VoiceFixer: Toward general speech restoration with neural vocoder</U></a>.
		  <cite>arXiv preprint arXiv:2109.13731</cite>.
	  <br>
	  <br>
	  </li>
		
	  <li>
      <strong>Zhao Y.</strong> (2020):
		  <a href="http://rave.ohiolink.edu/etdc/view?acc_num=osu1593462119759348" target="_blank"><U>Deep learning methods for reverberant and noisy speech enhancement</U></a>.
		  <cite>Dissertation, The Ohio State University</cite>.
      <br>
      <br>
	  </li>

	  <li>
      <strong>Zhao Y.</strong> and Wang D.L. (2020): <a href="https://cliffzhao.github.io/Publications/ZW.INTERSPEECH20.pdf" target="_blank"><U>Noisy-reverberant speech enhancement using DenseUNet with time-frequency attention</U></a>.
	    <cite>Proceedings of INTERSPEECH-20</cite>, pp. 3261-3265.
      <br>
      <br>
	  </li>

    <li>
      <strong>Zhao Y.</strong>, Wang D.L., Xu B., and Zhang T. (2020): 
		  <a href="https://cliffzhao.github.io/Publications/ZWXZ.taslp20.pdf" target="_blank"><U>Monaural speech dereverberation using temporal convolutional networks with self attention</U></a>.
	    <cite>IEEE/ACM Transactions on Audio, Speech, and Language Processing</cite>, vol. 28, pp. 1598-1607.
      <br>
      <br>
	  </li>

    <li>
      <strong>Zhao Y.</strong>, Wang Z.-Q., and Wang D.L. (2019): 
	    <a href="https://cliffzhao.github.io/Publications/ZWW.taslp19.pdf" target="_blank"><U>Two-stage deep learning for noisy-reverberant speech enhancement</U></a>.
      <cite>IEEE/ACM Transactions on Audio, Speech, and Language Processing</cite>, vol. 27, pp. 53-62.
      <br>
      <br>
		</li>

    <li>
      <strong>Zhao Y.</strong>, Wang D.L., Johnson E.M., and Healy E.W. (2018):
      <a href="https://cliffzhao.github.io/Publications/ZWJH.jasa18.pdf" target="_blank"><U>A deep learning based segregation algorithm to increase speech intelligibility for
hearing-impaired listeners in reverberant-noisy conditions</U></a>. <cite>Journal of the Acoustical Society of America</cite>, vol. 144, pp. 1627-1637.
	    <br>
	    <br>
    </li>

    <li>
      <strong>Zhao Y.</strong>, Wang D.L., Xu B., and Zhang T. (2018):
      <a href="https://cliffzhao.github.io/Publications/ZWXZ.icassp18.pdf" target="_blank"><U>Late reverberation suppression using recurrent neural networks with long short-term memory</U></a>. <cite>Proceedings of ICASSP-18</cite>, pp. 5434-5438.
	    <br>
	    <br>
  	</li>	    

    <li><strong>Zhao Y.</strong>, Xu B., Giri R., and Zhang T. (2018):
      <a href="https://cliffzhao.github.io/Publications/ZXGZ.icassp18.pdf" target="_blank"><U>Perceptually guided speech enhancement using deep neural networks</U></a>. <cite>Proceedings of ICASSP-18</cite>, pp. 5074-5078.
	    <br>
	    <br>
		</li>

    <li>
      <strong>Zhao Y.</strong>, Wang Z.-Q., and Wang D.L. (2017):
      <a href="https://cliffzhao.github.io/Publications/ZWW.icassp17.pdf" target="_blank"><U>A two-stage algorithm for noisy and reverberant speech enhancement</U></a>. <cite>Proceedings of ICASSP-17</cite>, pp. 5580-5584.
	    <br>
	    <br>
		</li>

    <li>
      <strong>Zhao Y.</strong>, Wang D.L., Merks I., and Zhang T. (2016): 
      <a href="https://cliffzhao.github.io/Publications/ZWMZ.icassp16.pdf" target="_blank"><U>DNN-based enhancement of noisy and reverberant speech</U></a>.
      <cite>Proceedings of ICASSP-16</cite>, pp. 6525-6529.
	    <br>
	    <br>
		</li>

    <li>
      Wang Z.-Q.,<strong> Zhao Y.</strong>, and Wang D.L. (2016): 
      <a href="https://cliffzhao.github.io/Publications/WZW.icassp16.pdf" target="_blank"><U>Phoneme-specific speech separation</U></a>.
      <cite>Proceedings of ICASSP-16</cite>, pp. 146-150.
	    <br>
	    <br>
    </li>
		
    </ul>
	
    <h2>Service</h2>
      <ul>
      <p><strong>Reviewer:</strong></p>
      <li>
        IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)
      </li>

      <li>
        IEEE Transactions on Multimedia (TMM)
      </li>

      <li>
        JASA Express Letters (JASA-EL)
      </li>

      <li>
        Journal of Speech, Language, and Hearing Research (JSLHR)
      </li>

	    <li>
        Computer Speech and Language
	    </li>
		
	    <li>
        ICASSP-2021
	    </li>

    </ul>


</div><!-- /.blurb -->
